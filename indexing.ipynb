{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.confluence import ConfluenceLoader\n",
    "\n",
    "from modules.indexing import load_docs, split_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set env vars for confluence wiki\n",
    "CONFLUENCE_PAT = os.getenv(\"CONFLUENCE_PAT\")\n",
    "CONFLUENCE_SPACE_KEY = os.getenv(\"CONFLUENCE_SPACE_KEY\")\n",
    "CONFLUENCE_URL = os.getenv(\"CONFLUENCE_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set env vars for indexing\n",
    "CHUNK_SIZE = 1024\n",
    "CHUNK_OVERLAP = 32\n",
    "# collection names in chroma will be based on the chunk size\n",
    "# thus you can experiment retrieving chunks of differnt size\n",
    "COLLECTION_NAME = f\"{CHUNK_SIZE}_{CONFLUENCE_SPACE_KEY}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and splitting documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Confluence document loader and \n",
    "# load documents from Confluence Wiki\n",
    "loader = ConfluenceLoader(\n",
    "    url=CONFLUENCE_URL,\n",
    "    token=CONFLUENCE_PAT,\n",
    "    cloud=False,\n",
    "    space_key=CONFLUENCE_SPACE_KEY,\n",
    "    include_attachments=False,\n",
    ")\n",
    "docs = load_docs(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: save a local copy of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "import textwrap\n",
    "\n",
    "class Metadata(TypedDict):\n",
    "    title: str\n",
    "    id: str\n",
    "    source: str\n",
    "    when: str\n",
    "\n",
    "def write_page_to_file(content: str, metadata: Metadata) -> None:\n",
    "    # Prepare the directory\n",
    "    base_directory = 'data/docu/ITI-CS'\n",
    "    \n",
    "    # Extract date from the 'when' key\n",
    "    date_part = metadata['when'].split('T')[0]  # Extract the date from the 'when' key\n",
    "    \n",
    "    # Format the filename by sanitizing the title to remove any special characters that could form paths\n",
    "    safe_title = metadata['title'].replace('/', '_').replace('\\\\', '_')\n",
    "    \n",
    "    # Format the filename based on title, id, and date\n",
    "    file_name = f\"{safe_title}_{metadata['id']}_{date_part}.txt\"\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(base_directory, exist_ok=True)\n",
    "    \n",
    "    # Full path to the file\n",
    "    full_path = os.path.join(base_directory, file_name)\n",
    "    \n",
    "    # Wrap content to 80 characters wide\n",
    "    wrapped_content = textwrap.fill(content, width=80)\n",
    "    \n",
    "    # Write the content to the file\n",
    "    with open(full_path, 'w') as file:\n",
    "        file.write(wrapped_content)\n",
    "\n",
    "for doc in docs:\n",
    "    write_page_to_file(content=doc.page_content, metadata=doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: recursive character text splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 291 documents into 845 chunks.\n"
     ]
    }
   ],
   "source": [
    "# initialize splitter and split docs into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP, add_start_index=True\n",
    ")\n",
    "chunks = split_documents(splitter, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize client for ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  a client that connects to a local chromadb server\n",
    "chroma_settings = Settings(allow_reset=True)\n",
    "chroma_client = chromadb.HttpClient(\n",
    "    settings=chroma_settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and save embeddings in ChromaDB\n",
    "\n",
    "Choose one of the options for creating embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Bedrock (embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.bedrock import BedrockEmbeddings\n",
    "embeddings_function = BedrockEmbeddings(\n",
    "    credentials_profile_name=os.getenv(\"AWS_CREDENTIALS_PROFILE_NAME\"),\n",
    "    region_name=os.getenv(\"AWS_REGION_NAME\", \"eu-central-1\"),\n",
    "    model_id=os.getenv(\"AWS_EMBEDDING_MODEL_ID\", \"amazon.titan-text-express-v1\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama embeddings\n",
    "\n",
    "- [Blog post about embedding models (by ollama)](https://ollama.com/blog/embedding-models)\n",
    "- [Ollama embedding model (langchain docs)](https://python.langchain.com/docs/integrations/text_embedding/ollama/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "\n",
    "OLLAMA_EMBEDDING_MODEL = os.getenv(\"OLLAMA_EMBEDDING_MODEL\", \"mxbai-embed-large\")\n",
    "embeddings_function = OllamaEmbeddings(model=OLLAMA_EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actually generate and save embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_client.get_or_create_collection(name=COLLECTION_NAME)\n",
    "if collection.count() <= 0:\n",
    "    # store each document in a vector embedding database\n",
    "    for d in chunks:\n",
    "        response = embeddings_function.embed_query(d.page_content)\n",
    "        collection.add(\n",
    "            ids=[str(uuid.uuid1())],\n",
    "            embeddings=[response],\n",
    "            documents=[d.page_content],\n",
    "            metadatas=[d.metadata]\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
