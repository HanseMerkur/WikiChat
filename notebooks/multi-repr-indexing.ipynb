{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi representation indexing\n",
    "\n",
    "Here we don't chunk up the documents. Instead we create summaries with important keywords, which are optimized for retrieval. We store the full document corresponding to the summary in a redis DB. Using a MultiVectorRetriever, we search for similar documents using the embedded summaries. But ultimately, the full document (parent document of the summary) is returned. Thus, we can later take advantage of LLMs with large context windows (like Amazon Titan Text Express wih 8k tokens) and provide them the full document to answer a users question, instead of a set of chunks, which may not contain sufficient information.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "\n",
    "Provide `PYTHONPATH` (and other environment variables) in a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "# set env vars for confluence wiki\n",
    "CONFLUENCE_PAT = os.getenv(\"CONFLUENCE_PAT\")\n",
    "CONFLUENCE_SPACE_KEY = os.getenv(\"CONFLUENCE_SPACE_KEY\")\n",
    "CONFLUENCE_URL = os.getenv(\"CONFLUENCE_URL\")\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.confluence import ConfluenceLoader\n",
    "from modules.indexing import load_docs\n",
    "\n",
    "# initialize Confluence document loader and \n",
    "# load documents from Confluence Wiki\n",
    "loader = ConfluenceLoader(\n",
    "    url=CONFLUENCE_URL,\n",
    "    token=CONFLUENCE_PAT,\n",
    "    cloud=False,\n",
    "    space_key=CONFLUENCE_SPACE_KEY,\n",
    "    include_attachments=False,\n",
    ")\n",
    "confluence_docs = load_docs(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create document summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws.chat_models import ChatBedrock\n",
    "\n",
    "# initialize bedrock LLM\n",
    "bedrock_llm = ChatBedrock(\n",
    "    credentials_profile_name=os.getenv(\"AWS_CREDENTIALS_PROFILE_NAME\"),\n",
    "    region_name=os.getenv(\"AWS_REGION_NAME\", \"eu-central-1\"),\n",
    "    model_id=os.getenv(\"AWS_LANGUAGE_MODEL_ID\", \"amazon.titan-text-express-v1\"),\n",
    "    model_kwargs={\"temperature\": 0.0, \"maxTokenCount\": 2048}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from modules.indexing import SUMMARY_PROMPT_TEMPLATE_DE\n",
    "\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(SUMMARY_PROMPT_TEMPLATE_DE)\n",
    "    | bedrock_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# create summaries\n",
    "summaries = chain.batch(confluence_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import BedrockEmbeddings\n",
    "\n",
    "# init embeddings function (bedrock)\n",
    "bedrock_embeddings = BedrockEmbeddings(\n",
    "    credentials_profile_name=os.getenv(\"AWS_CREDENTIALS_PROFILE_NAME\"),\n",
    "    region_name=os.getenv(\"AWS_REGION_NAME\", \"eu-central-1\"),\n",
    "    model_id=os.getenv(\"AWS_EMBEDDING_MODEL_ID\", \"amazon.titan-text-express-v1\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index summaries and save parent docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb import HttpClient\n",
    "from chromadb.config import Settings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# set collection name for the summaries\n",
    "COLLECTION_NAME = f\"{CONFLUENCE_SPACE_KEY}_concise_summaries\"\n",
    "\n",
    "# we need to init a client that connects to a local chromadb server\n",
    "chroma_settings = Settings(allow_reset=True)\n",
    "chroma_client = HttpClient(\n",
    "    settings=chroma_settings,\n",
    ")\n",
    "\n",
    "# init the vectorstore to use to index the summaries\n",
    "vectorstore = Chroma(\n",
    "    client=chroma_client,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding_function=bedrock_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init document store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage.redis import RedisStore\n",
    "from redis import Redis\n",
    "\n",
    "# init redis client first\n",
    "redis_client = Redis(host='localhost', port=6379)\n",
    "\n",
    "# init the storage layer for the parent documents (full confluence pages)\n",
    "doc_store = RedisStore(\n",
    "    client=redis_client,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init MultiVectorRetriever\n",
    "\n",
    "[MultiVectorRetriever](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector/) documentation from LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import MultiVectorRetriever\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# id key for summary document\n",
    "ID_KEY = \"parent_doc_id\"\n",
    "\n",
    "# init retriever\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=doc_store,\n",
    "    id_key=ID_KEY,\n",
    "    search_kwargs={\"k\": 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save embeddings and parent docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_doc_ids = [doc.metadata['id'] for doc in confluence_docs]\n",
    "\n",
    "# create a list of summary docs\n",
    "# each element is of type Document\n",
    "# - containing the summary as page_content\n",
    "# - having a metadata property with a \"parent_doc_id\" corresponding do the id of the parent doc (full conflunce doc)\n",
    "summary_docs = [\n",
    "    Document(page_content=summary, metadata={ID_KEY: parent_doc_ids[i]})\n",
    "    for i, summary in enumerate(summaries)\n",
    "]\n",
    "\n",
    "# Add embeddings of the summaries to the vector store\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "\n",
    "# add the parent document (full conflunce doc) and it's id to the document store\n",
    "retriever.docstore.mset(list(zip(parent_doc_ids, confluence_docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define question\n",
    "question = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from modules.retrieval import PROMPT_TEMPLATE_DE, format_docs\n",
    "import textwrap\n",
    "\n",
    "# perform similarity search on the embeddings of the summaries\n",
    "sub_docs = vectorstore.similarity_search(query=question, k=1)\n",
    "print(\"Summary of the most similar document:\")\n",
    "# Wrap content to 80 characters wide\n",
    "print(textwrap.fill(sub_docs[0].page_content, width=80))\n",
    "print(\"-------------------------------------\")\n",
    "\n",
    "# retrieve the parent document\n",
    "relevant_docs = retriever.invoke(input=question)\n",
    "print(\"Contents of the actual confluence document:\")\n",
    "print(textwrap.fill(relevant_docs[0].page_content, width=80))\n",
    "\n",
    "context_text = format_docs(relevant_docs)\n",
    "prompt = ChatPromptTemplate.from_template(PROMPT_TEMPLATE_DE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Bedrock\n",
    "\n",
    "We already initialized a Bedrock LLM when we creted the summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Ollama\n",
    "\n",
    "Choose a model with a context windof of at least 2048 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "\n",
    "model_name = \"mistral:7b\"\n",
    "ChatOllama(model=model_name, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = prompt | bedrock_llm | StrOutputParser()\n",
    "resp = chain.invoke({\"context\": context_text, \"question\": question})\n",
    "print(textwrap.fill(resp, width=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# print source\n",
    "print(f\"Title: {relevant_docs[0].metadata['title']}\")\n",
    "print(f\"Link: {relevant_docs[0].metadata['source']}\")\n",
    "\n",
    "def convert_iso_to_readable(date_iso):\n",
    "    # Parsing the ISO 8601 date string\n",
    "    date_obj = datetime.fromisoformat(date_iso)\n",
    "    \n",
    "    # Formatting to a more readable form, e.g., \"May 2, 2024, 7:12 PM\"\n",
    "    readable_date = date_obj.strftime(\"%B %d, %Y, %I:%M %p\")\n",
    "    \n",
    "    return readable_date\n",
    "\n",
    "print(f\"Last edited: {convert_iso_to_readable(relevant_docs[0].metadata['when'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
