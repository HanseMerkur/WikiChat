{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic indexing\n",
    "\n",
    "Here, we split the confluence documents recursively by character into smaller pieces (chunks), embed the resulting chunks and store the embeddings in a Chroma database. Thus we can later find chunks similar to the query and provide them to an LLM as context. This is the most basic approach to document splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.confluence import ConfluenceLoader\n",
    "\n",
    "from modules.indexing import load_docs, split_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "Provide `PYTHONPATH` (and other environment variables) in a `.env` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set env vars for confluence wiki\n",
    "CONFLUENCE_PAT = os.getenv(\"CONFLUENCE_PAT\")\n",
    "CONFLUENCE_SPACE_KEY = os.getenv(\"CONFLUENCE_SPACE_KEY\")\n",
    "CONFLUENCE_URL = os.getenv(\"CONFLUENCE_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set env vars for indexing\n",
    "CHUNK_SIZE = 1024\n",
    "CHUNK_OVERLAP = 128\n",
    "# collection names in chroma will be based on the chunk size\n",
    "# thus you can experiment retrieving chunks of differnt size\n",
    "COLLECTION_NAME = f\"{CHUNK_SIZE}_{CONFLUENCE_SPACE_KEY}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading documents from confluence wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Confluence document loader and \n",
    "# load documents from Confluence Wiki\n",
    "loader = ConfluenceLoader(\n",
    "    url=CONFLUENCE_URL,\n",
    "    token=CONFLUENCE_PAT,\n",
    "    cloud=False,\n",
    "    space_key=CONFLUENCE_SPACE_KEY,\n",
    "    include_attachments=False,\n",
    ")\n",
    "docs = load_docs(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: save a local copy of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "import textwrap\n",
    "\n",
    "class Metadata(TypedDict):\n",
    "    title: str\n",
    "    id: str\n",
    "    source: str\n",
    "    when: str\n",
    "\n",
    "def write_page_to_file(content: str, metadata: Metadata) -> None:\n",
    "    # Prepare the directory\n",
    "    base_directory = f'../data/docu/{CONFLUENCE_SPACE_KEY}'\n",
    "    \n",
    "    # Extract date from the 'when' key\n",
    "    date_part = metadata['when'].split('T')[0]  # Extract the date from the 'when' key\n",
    "    \n",
    "    # Format the filename by sanitizing the title to remove any special characters that could form paths\n",
    "    safe_title = metadata['title'].replace('/', '_').replace('\\\\', '_')\n",
    "    \n",
    "    # Format the filename based on title, id, and date\n",
    "    file_name = f\"{safe_title}_{metadata['id']}_{date_part}.txt\"\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(base_directory, exist_ok=True)\n",
    "    \n",
    "    # Full path to the file\n",
    "    full_path = os.path.join(base_directory, file_name)\n",
    "    \n",
    "    # Wrap content to 80 characters wide\n",
    "    wrapped_content = textwrap.fill(content, width=80)\n",
    "    \n",
    "    # Write the content to the file\n",
    "    with open(full_path, 'w') as file:\n",
    "        file.write(wrapped_content)\n",
    "\n",
    "for doc in docs:\n",
    "    write_page_to_file(content=doc.page_content, metadata=doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive character text splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize splitter and split docs into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP, add_start_index=True\n",
    ")\n",
    "chunks = split_documents(splitter, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize client for ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  a client that connects to a local chromadb server\n",
    "chroma_settings = Settings(allow_reset=True)\n",
    "chroma_client = chromadb.HttpClient(\n",
    "    settings=chroma_settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and save embeddings in ChromaDB\n",
    "\n",
    "Choose one of the options for creating embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Bedrock (embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.bedrock import BedrockEmbeddings\n",
    "\n",
    "embeddings_function = BedrockEmbeddings(\n",
    "    credentials_profile_name=os.getenv(\"AWS_CREDENTIALS_PROFILE_NAME\"),\n",
    "    region_name=os.getenv(\"AWS_REGION_NAME\", \"eu-central-1\"),\n",
    "    model_id=os.getenv(\"AWS_EMBEDDING_MODEL_ID\", \"amazon.titan-text-express-v1\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama embeddings\n",
    "\n",
    "- [Blog post about embedding models (by ollama)](https://ollama.com/blog/embedding-models)\n",
    "- [Ollama embedding model (langchain docs)](https://python.langchain.com/docs/integrations/text_embedding/ollama/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "\n",
    "OLLAMA_EMBEDDING_MODEL = os.getenv(\"OLLAMA_EMBEDDING_MODEL\", \"mxbai-embed-large\")\n",
    "embeddings_function = OllamaEmbeddings(model=OLLAMA_EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actually generate and save embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(name=COLLECTION_NAME)\n",
    "if collection.count() <= 0:\n",
    "    for d in chunks:\n",
    "        response = embeddings_function.embed_query(d.page_content)\n",
    "        collection.add(\n",
    "            ids=[str(uuid.uuid1())],\n",
    "            embeddings=[response],\n",
    "            documents=[d.page_content],\n",
    "            metadatas=[d.metadata]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "chroma_settings = Settings(allow_reset=True)\n",
    "chroma_client = chromadb.HttpClient(settings=chroma_settings)\n",
    "\n",
    "db = Chroma(\n",
    "    client=chroma_client, collection_name=COLLECTION_NAME, embedding_function=embeddings_function\n",
    ")\n",
    "\n",
    "question = \"Was macht der Policy Reporter?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.retrieval import format_docs\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from modules.retrieval import PROMPT_TEMPLATE_DE\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "relevant_docs = retriever.invoke(input=question)\n",
    "\n",
    "context_text = format_docs(relevant_docs)\n",
    "prompt = ChatPromptTemplate.from_template(PROMPT_TEMPLATE_DE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws.chat_models import ChatBedrock\n",
    "\n",
    "CREDENTIALS_PROFILE_NAME = os.getenv(\"AWS_CREDENTIALS_PROFILE_NAME\")\n",
    "REGION_NAME = os.getenv(\"AWS_REGION_NAME\", \"eu-central-1\")\n",
    "AWS_LANGUAGE_MODEL_ID = os.getenv(\n",
    "    \"AWS_LANGUAGE_MODEL_ID\", \"amazon.titan-text-express-v1\"\n",
    ")\n",
    "AWS_EMBEDDING_MODEL_ID = os.getenv(\n",
    "    \"AWS_EMBEDDING_MODEL_ID\", \"amazon.titan-embed-text-v1\"\n",
    ")\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    credentials_profile_name=CREDENTIALS_PROFILE_NAME,\n",
    "    region_name=REGION_NAME,\n",
    "    model_id=AWS_LANGUAGE_MODEL_ID,\n",
    "    model_kwargs={\"temperature\": 0.0, \"maxTokenCount\": 2048},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "\n",
    "model_name = \"mistral:7b\"\n",
    "\n",
    "ChatOllama(model=model_name, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import textwrap\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "resp = chain.invoke({\"context\": context_text, \"question\": question})\n",
    "print(textwrap.fill(resp, width=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def convert_iso_to_readable(date_iso):\n",
    "    # Parsing the ISO 8601 date string\n",
    "    date_obj = datetime.fromisoformat(date_iso)\n",
    "    \n",
    "    # Formatting to a more readable form, e.g., \"May 2, 2024, 7:12 PM\"\n",
    "    readable_date = date_obj.strftime(\"%B %d, %Y, %I:%M %p\")\n",
    "    \n",
    "    return readable_date\n",
    "\n",
    "print(\"Sources:\\n\")\n",
    "for doc in relevant_docs:\n",
    "   # print source\n",
    "    print(f\"Title: {doc.metadata['title']}\")\n",
    "    print(f\"Link: {doc.metadata['source']}\")\n",
    "    print(f\"Last edited: {convert_iso_to_readable(doc.metadata['when'])}\")\n",
    "    print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
